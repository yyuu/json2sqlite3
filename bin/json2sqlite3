#!/usr/bin/env bash

set -euo pipefail

[[ -z "${DEBUG:-}" ]] || set -x

TMPDIR="$(mktemp -d)"
VERSION="0.20230523.1"
VERBOSITY=2

on_exit() {
  rm -fr "${TMPDIR}"
}

trap on_exit EXIT

error() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-0}" -ge 0 ]]; then
    { if [[ -t 2 ]]; then printf '\033[1;31m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

warn() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-1}" -ge 1 ]]; then
    { if [[ -t 2 ]]; then printf '\033[1;33m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

info() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-2}" -ge 2 ]]; then
    { if [[ -t 2 ]]; then printf '\033[0;32m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

debug() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-3}" -ge 3 ]]; then
    { if [[ -t 2 ]]; then printf '\033[0;37m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

validate_ident() {
  local ident
  for ident in "$@"; do
    if [[ ! "${ident:-}" =~ ^[A-Z_a-z][0-9A-Z_a-z]*$ ]]; then
      warn "${0##*/}: invalid identifier for SQLite3 tables/columns: ${ident:-}"
      return 1
    fi
  done
}

ppq() {
  shift 1 # skipping database file
  local arg # expecting SQL statements to follow...
  for arg in "$@"; do
    local continued_p=
    local line
    while IFS='' read -r line; do
      if [[ -z "${continued_p:-}" ]]; then
        echo "sqlite3> ${line:-}"
        if [[ "${line:-}" == "."* ]] || [[ "${line:-}" == *";" ]]; then
          :
        else
          continued_p=1
        fi
      else
        echo "    ...> ${line:-}"
      fi
    done <<< "${arg}"
  done
}

runq() {
  if [[ -n "${VERBOSE:-}" ]]; then
    ppq "$@" | debug
  fi
  if [[ -z "${DRY_RUN:-}" ]]; then
    sqlite3 "$@"
  fi
}

compare_column_specs() {
  local control_column_specs="$1"
  local test_column_specs="$2"
  local cmdout
  cmdout="$(mktemp "${TMPDIR}/cmdout.XXXXXXXX")"
  local control_column_names
  local test_column_names
  control_column_names="$(mktemp "${TMPDIR}/tmp.XXXXXXXX")"
  test_column_names="$(mktemp "${TMPDIR}/tmp.XXXXXXXX")"
  jq --raw-output 'map([.name, .type, (if .pk == 1 then "PRIMARY KEY" else "" end)]) | sort[] | @tsv' <<< "${control_column_specs}" > "${control_column_names}"
  jq --raw-output 'map([.name, .type, (if .pk == 1 then "PRIMARY KEY" else "" end)]) | sort[] | @tsv' <<< "${test_column_specs}" > "${test_column_names}"
  if diff -u "${control_column_names}" "${test_column_names}"; then
    :
  else
    exit_status="$?"
    { echo "${0##*/:}: detected schema drift on table column(s)..."
      cat "${cmdout}"
    } | info
    return "${exit_status:-1}"
  fi
}

detect_column_specs_from_args() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local arg
  local cid=1
  local specs='[]'
  for arg in "$@"; do
    local column_name="${arg%%:*}"
    local column_type="${arg#*:}"
    validate_ident "${column_name}"
    local pk='false'
    if [[ "${column_name:-}" == "${pk:-}" ]]; then
      pk='true'
    fi
    specs="$(jq \
      --argjson cid "${cid}" \
      --argjson notnull 'false' \
      --argjson dflt_value 'null' \
      --argjson pk "${pk:-false}" \
      --arg name "${column_name}" \
      --arg type "${column_type}" \
      --compact-output \
      '. + [{cid: $cid, name: $name, type: $type, notnull: $notnull, dflt_value: $dflt_value, pk: $pk}]' \
      <<< "${specs}" \
    )"
    cid="$(( cid + 1 ))"
  done
  echo "${specs}"
}

detect_column_specs_from_json() {
  local json_file="$1"
  local csv_file
  csv_file="$(mktemp "${TMPDIR}/csv_file.XXXXXXXX")"
  jq --raw-output 'map(
    to_entries | map(
      (.value | type) as $json_type |
        (if "array" == $json_type then
          "JSON"
        elif "boolean" == $json_type then
          "INTEGER"
        elif "null" == $json_type then
          "JSON"
        elif "number" == $json_type then
          "NUMERIC"
        elif "object" == $json_type then
          "JSON"
        elif "string" == $json_type then
          "TEXT"
        else
          error("unsupported JSON data type: \(.value):\($json_type)")
        end) as $sqlite_type | [.key, $sqlite_type]
      )
  ) | add[] | @csv' < "${json_file}" > "${csv_file}"
  sqlite3 "/dev/null" \
    "CREATE TEMPORARY TABLE _s1 (name TEXT, type TEXT);" \
    "CREATE TEMPORARY TABLE s1 (cid INTEGER PRIMARY KEY, name TEXT, type TEXT);" \
    "CREATE UNIQUE INDEX s1_name ON s1 (name);" \
    ".mode csv" \
    ".import '${csv_file}' _s1" \
    ".mode ascii" \
    "INSERT OR IGNORE INTO s1 (name, type) SELECT name, type FROM _s1;" \
    "SELECT json_group_array(json_object('cid', cid, 'name', name, 'type', type, 'notnull', FALSE, 'dflt_value', NULL, 'pk', FALSE)) FROM s1 ORDER BY cid;"
}

detect_column_specs_from_table() {
  local database_file="$1"
  local table_name="$2"
  sqlite3 "${database_file}" \
    ".parameter set @table_name '${table_name}'" \
    "SELECT json_group_array(json_object('cid', cid, 'name', name, 'type', type, 'notnull', \"notnull\", 'dflt_value', dflt_value, 'pk', pk)) FROM pragma_table_info(@table_name) ORDER BY cid;"
}

build_create_table_statement() {
  local pk
  local temporary_p
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    "--temporary" ) temporary_p=1 ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local create_table_specs="$3"
  local stmt
  if [[ -z "${temporary_p:-}" ]]; then
    stmt="CREATE TABLE \"${table_name}\" ("
  else
    stmt="CREATE TEMPORARY TABLE \"${table_name}\" ("
  fi
  local _column
  while IFS='' read -r _column; do
    column_name="${_column%%:*}"
    column_type="${_column#*:}"
    validate_ident "${column_name}"
    if [[ "${column_name}" == "${pk:-}" ]]; then
      column_type+=" PRIMARY KEY"
    fi
    stmt+=$'\n'
    stmt+="  \"${column_name}\" ${column_type},"
  done < <(jq --raw-output 'map("\(.name):\(.type)")[]' <<< "${create_table_specs}")
  stmt="${stmt%,}"
  stmt+=$'\n)'
  echo "${stmt};"
}

build_insert_into_table_from_table_statement() {
  local insert_or_replace_p
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--insert-or-replace" ) insert_or_replace_p=1 ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local source_table_name="$3"
  local table_column_specs="$4"
  local stmt
  if [[ -n "${insert_or_replace_p:-}" ]]; then
    stmt="INSERT OR REPLACE INTO \"${table_name}\" ("
  else
    stmt="INSERT INTO \"${table_name}\" ("
  fi
  local _column
  while IFS='' read -r _column; do
    validate_ident "${_column%%:*}"
    stmt+=$'\n'
    stmt+="  \"${_column%%:*}\","
  done < <(jq --raw-output 'map("\(.name):\(.type)")[]' <<< "${table_column_specs}")
  stmt="${stmt%,}"
  stmt+=$'\n) SELECT'
  while IFS='' read -r _column; do
    validate_ident "${_column%%:*}"
    stmt+=$'\n'
    stmt+="  \"${_column%%:*}\","
  done < <(jq --raw-output 'map("\(.name):\(.type)")[]' <<< "${table_column_specs}")
  stmt="${stmt%,}"
  stmt+=$'\nFROM\n'
  stmt+="  \"${source_table_name}\";"
  echo "${stmt}"
}

reconcile_column_specs3() {
  local spec1="$1"
  local spec2="$2"
  local spec3="$3"
  local s1
  local s2
  local s3
  s1="$(mktemp "${TMPDIR}/s1.XXXXXXXX")"
  s2="$(mktemp "${TMPDIR}/s2.XXXXXXXX")"
  s3="$(mktemp "${TMPDIR}/s3.XXXXXXXX")"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec1}" > "${s1}"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec2}" > "${s2}"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec3}" > "${s3}"
  sqlite3 /dev/null \
    "CREATE TEMPORARY TABLE s1 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    "CREATE TEMPORARY TABLE s2 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    "CREATE TEMPORARY TABLE s3 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    ".mode csv" \
    ".import '${s1}' s1" \
    ".import '${s2}' s2" \
    ".import '${s3}' s3" \
    ".mode ascii" \
    "WITH names AS (
      SELECT
        name
      FROM
        s1
      UNION SELECT
        name
      FROM
        s2
      UNION SELECT
        name
      FROM
        s3
    ), s0 AS (
      SELECT
        ( CASE
          WHEN s1.cid IS NOT NULL THEN s1.cid
          WHEN s2.cid IS NOT NULL THEN s2.cid
          ELSE s3.cid
          END ) AS cid,
        names.name,
        ( CASE
          WHEN s1.type IS NOT NULL THEN s1.type
          WHEN s2.type IS NOT NULL THEN s2.type
          ELSE s3.type
          END ) AS type,
        ( CASE
          WHEN s1.\"notnull\" IS NOT NULL THEN s1.\"notnull\"
          WHEN s2.\"notnull\" IS NOT NULL THEN s2.\"notnull\"
          ELSE s3.\"notnull\"
          END ) AS \"notnull\",
        ( CASE
          WHEN s1.dflt_value IS NOT NULL THEN s1.dflt_value
          WHEN s2.dflt_value IS NOT NULL THEN s2.dflt_value
          ELSE s3.dflt_value
          END ) AS dflt_value,
        ( CASE
          WHEN s1.pk IS NOT NULL THEN s1.pk
          WHEN s2.pk IS NOT NULL THEN s2.pk
          ELSE s3.pk
          END ) AS pk
      FROM
        names
      LEFT JOIN
        s1
          ON names.name = s1.name
      LEFT JOIN
        s2
          ON names.name = s2.name
      LEFT JOIN
        s3
          ON names.name = s3.name
      ORDER BY
        cid
    ) SELECT
      json_group_array(
        json_object(
          'cid', cid,
          'name', name,
          'type', type,
          'notnull', \"notnull\",
          'dflt_value', dflt_value,
          'pk', pk
        )
      )
    FROM
      s0;"
}

alter_table() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local old_column_specs="$3"
  local new_column_specs="$4"

  local backup_table_name
  backup_table_name="_${table_name}_backup$(date '+%s')"
  runq_args=(
    "BEGIN TRANSACTION;"
    "DROP TABLE IF EXISTS \"${backup_table_name}\";"
    "ALTER TABLE \"${table_name}\" RENAME TO \"${backup_table_name}\";"
  )
  runq_args+=("$(build_create_table_statement --pk="${pk:-}" "${database_file}" "${table_name}" "${new_column_specs}")")
  runq_args+=("$(build_insert_into_table_from_table_statement --insert-or-replace "${database_file}" "${table_name}" "${backup_table_name}" "${old_column_specs}")")
  # TODO: DROP TABLE IF EXISTS "${backup_table_name}"...
  runq_args+=("COMMIT TRANSACTION;")
  runq "${database_file}" "${runq_args[@]}"
}

create_table() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local table_column_specs="$3"
  local stmt
  stmt="$(build_create_table_statement --pk="${pk:-}" "${database_file}" "${table_name}" "${table_column_specs}" || true)"
  if [[ -z "${stmt:-}" ]]; then
    return 1
  fi
  runq "${database_file}" "${stmt}"
}

arg_after_queries=()
arg_alter_table=1
arg_before_queries=()
arg_column_specs=() # contains columns including special columns e.g. primary key, created/updated/deleted
arg_create_table=1
arg_created_column=
arg_deleted_column=
arg_insert_if_empty=
arg_output=
arg_preserve_created=
arg_primary_key_column=
arg_soft_delete=
arg_updated_column=

while [[ $# -gt 0 ]]; do
  case "${1:-}" in
  "--" )
    break
    ;;
  "--after-query" | "--after-query="* )
    if [[ "$1" == *"="* ]]; then
      arg_after_queries+=("${1#*=}")
    else
      arg_after_queries+=("${2:-}")
      shift 1
    fi
    ;;
  "--alter-table" | "--no-alter-table" )
    if [[ "$1" == "--no-"* ]]; then
      arg_alter_table=
    else
      arg_alter_table=1
    fi
    ;;
  "--before-query" | "--before-query="* )
    if [[ "$1" == *"="* ]]; then
      arg_before_queries+=("${1#*=}")
    else
      arg_before_queries+=("${2:-}")
      shift 1
    fi
    ;;
  "--create-table" | "--no-create-table" )
    if [[ "$1" == "--no-"* ]]; then
      arg_create_table=
    else
      arg_create_table=1
    fi
    ;;
  "--created-column" | "--created-column="* )
    if [[ "$1" == *"="* ]]; then
      arg_created_column="${1#*=}"
    else
      arg_created_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_created_column:-}" != *":"* ]]; then
      arg_created_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_created_column}")
    ;;
  "--deleted-column" | "--deleted-column="* )
    if [[ "$1" == *"="* ]]; then
      arg_deleted_column="${1#*=}"
    else
      arg_deleted_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_deleted_column:-}" != *":"* ]]; then
      arg_deleted_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_deleted_column}")
    ;;
  "--dry-run" )
    DRY_RUN=1
    ;;
  "--generic-column" | "--generic-column="* )
    if [[ "$1" == *"="* ]]; then
      arg_generic_column="${1#*=}"
    else
      arg_generic_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_generic_column:-}" != *":"* ]]; then
      arg_generic_column+=":JSON"
    fi
    arg_column_specs+=("${generic_column}")
    ;;
  "--help" | "-h" )
    echo "${0##*/} [OPTIONS] DATABASE_FILE:TABLE_NAME < JSON"
    exit 0
    ;;
  "--insert-if-empty" | "--insert-if-empty="* )
    if [[ "$1" == *"="* ]]; then
      arg_insert_if_empty="${1#*=}"
    else
      arg_insert_if_empty="${2:-}"
      shift 1
    fi
    ;;
  "--primary-key-column" | "--primary-key-column="* )
    if [[ "$1" == *"="* ]]; then
      arg_primary_key_column="${1#*=}"
    else
      arg_primary_key_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_primary_key_column:-}" != *":"* ]]; then
      arg_primary_key_column+=":JSON"
    fi
    arg_column_specs+=("${arg_primary_key_column}")
    ;;
  "--preserve-created" | "--no-preserve-created" )
    if [[ "$1" == "--no-"* ]]; then
      arg_preserve_created=
    else
      arg_preserve_created=1
    fi
    ;;
  "--soft-delete" | "--no-soft-delete" )
    if [[ "$1" == "--no-"* ]]; then
      arg_soft_delete=
    else
      arg_soft_delete=1
    fi
    ;;
  "--updated-column" | "--updated-column="* )
    if [[ "$1" == *"="* ]]; then
      arg_updated_column="${1#*=}"
    else
      arg_updated_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_updated_column:-}" != *":"* ]]; then
      arg_updated_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_updated_column}")
    ;;
  "--version" )
    echo "${0##*/} ${VERSION:-n/a}"
    exit 0
    ;;
  "--verbose" | "-v" )
    VERBOSE=1
    VERBOSITY="$(( VERBOSITY + 1 ))"
    ;;
  * )
    if [[ -z "${arg_output:-}" ]]; then
      arg_output="${1:-}"
    else
      error "${0##*/}: error: unrecognized argument: ${1:-}"
      exit 1
    fi
    ;;
  esac
  shift 1
done

if ! command -v jq 1>/dev/null 2>&1; then
  error "${0##*/}: jq is not available"
  exit 1
fi

if ! command -v sqlite3 1>/dev/null 2>&1; then
  error "${0##*/}: sqlite3 is not available"
  exit 1
fi

if [[ "${arg_output:-}" != *":"* ]]; then
  error "${0##*/}: SQLite3 database and table were not given"
  exit 1
else
  database_file="${arg_output%:*}"
  table_name="${arg_output##*:}"
  validate_ident "${table_name}"
fi

if [[ -z "${arg_primary_key_column:-}" ]]; then
  pk_column_name=
else
  pk_column_name="${arg_primary_key_column%%:*}" # extract type info
  validate_ident "${pk_column_name}"
fi

# read from standard input
json_file="$(mktemp "${TMPDIR}/json_file.XXXXXXXX")"
cat > "${json_file}"

# fail early if the input data is not well-formed
json_file_length=0
if [[ -s "${json_file}" ]]; then
  # SQLite3's 'json' mode will produce records in array<object> schema.
  # Here this is expecting that the input data is structured in the same way.
  json_file_type="$(jq --raw-output 'type' < "${json_file}" || true)"
  if [[ "${json_file_type:-}" == "array" ]]; then
    json_file_length="$(jq --raw-output 'length' < "${json_file}" || true)"
  else
    echo "${0##*/}: error: input data is not an array: ${json_file}" 1>&2
    exit 1
  fi
else
  json_file_length=0
fi

# managing table schema...
table_description=""
if sqlite3 "${database_file}" "SELECT 1 FROM \"${table_name}\" LIMIT 1;" 1>/dev/null 2>&1; then
  debug "${0##*/}: detected existing table. inspecting schema information..."
  current_column_specs="$(detect_column_specs_from_table "${database_file}" "${table_name}" || true)"
  if [[ -z "${current_column_specs:-}" ]]; then
    error "${0##*/}: failed to inspect schema information from existing table: ${table_name}"
    exit 1
  fi
  if [[ ${#arg_column_specs[*]} -eq 0 ]]; then
    specified_column_specs='[]'
  else
    specified_column_specs="$(detect_column_specs_from_args --pk="${pk_column_name:-}" "${arg_column_specs[@]}" || true)"
    if [[ -z "${specified_column_specs:-}" ]]; then
      error "${0##*/}: failed to parse column specifications from arguments"
      exit 1
    fi
  fi
  detected_column_specs="$(detect_column_specs_from_json "${json_file}" || true)"
  if [[ -z "${detected_column_specs:-}" ]]; then
    error "${0##*/}: failed to detect column specifications from input data"
    exit 1
  fi
  reconciled_column_specs="$(reconcile_column_specs3 "${current_column_specs}" "${specified_column_specs}" "${detected_column_specs}" || true)"
  if [[ -z "${reconciled_column_specs:-}" ]]; then
    error "${0##*/}: failed to reconcile conflict on column specifications"
    exit 1
  fi
  if compare_column_specs "${current_column_specs}" "${reconciled_column_specs}"; then
    table_description="existing table"
    info "${0##*/}: existing table '${table_name}' has schema compatible with columns detected in importing data." 1>&2
  else
    if [[ -z "${arg_alter_table:-}" ]]; then
      error "${0##*/}: existing table '${table_name}' has schema drift with columns detected in importing data. use --alter-table option to alter table." 1>&2
      exit 1
    else
      table_description="existing table with modifying schema"
      warn "${0##*/}: existing table '${table_name}' has schema drift with columns detected in importing data. will alter table..." 1>&2
      if ! alter_table --pk="${pk_column_name:-}" "${database_file}" "${table_name}" "${current_column_specs}" "${reconciled_column_specs}"; then
        error "${0##*/}: failed to alter table: ${table_name}"
        exit 1
      fi
    fi
  fi
else
  debug "${0##*/}: not detected existing table. preparing schema information to create new table..."
  if [[ -z "${arg_create_table:-}" ]]; then
    error "${0##*/}: table '${table_name}' does not exist. use --create-table option to create table." 1>&2
    exit 1
  else
    table_description="new table"
    if [[ ${#arg_column_specs[*]} -eq 0 ]]; then
      specified_column_specs='[]'
    else
      specified_column_specs="$(detect_column_specs_from_args --pk="${pk_column_name:-}" "${arg_column_specs[@]}" || true)"
      if [[ -z "${specified_column_specs:-}" ]]; then
        error "${0##*/}: failed to parse column specifications from arguments"
        exit 1
      fi
    fi
    detected_column_specs="$(detect_column_specs_from_json "${json_file}" || true)"
    if [[ -z "${detected_column_specs:-}" ]]; then
      error "${0##*/}: failed to detect column specifications from input data"
      exit 1
    fi
    reconciled_column_specs="$(reconcile_column_specs3 '[]' "${specified_column_specs}" "${detected_column_specs}" || true)"
    if [[ -z "${reconciled_column_specs:-}" ]]; then
      error "${0##*/}: failed to reconcile conflict on column specifications"
      exit 1
    fi
    warn "${0##*/}: creating new table '${table_name}'" 1>&2
    if ! create_table --pk="${pk_column_name:-}" "${database_file}" "${table_name}" "${reconciled_column_specs}"; then
      error "${0##*/}: failed to create table: ${table_name}"
      exit 1
    fi
  fi
fi

current_column_specs="$(detect_column_specs_from_table "${database_file}" "${table_name}" || true)"
if [[ -z "${current_column_specs:-}" ]]; then
  error "${0##*/}: failed to inspect table schema: ${table_name}"
  exit 1
fi

if [[ "${json_file_length:-0}" -eq 0 ]]; then
  if [[ -z "${arg_insert_if_empty:-}" ]]; then
    info "${0##*/}: empty file. skipping importing: ${json_file}" 1>&2
    exit 0
  else
    if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_created_column:-}" ]] || [[ -z "${arg_updated_column:-}" ]] || [[ -z "${arg_deleted_column:-}" ]]; then
      error "${0##*/}: you need to specify --primary-key-column, --created-column, --updated-column and --deleted-column to insert negative cache record"
      exit 1
    fi
    validate_ident "${arg_primary_key_column%%:*}" "${arg_created_column%%:*}" "${arg_updated_column%%:*}" "${arg_deleted_column%%:*}"
    info "${0##*/}: empty file. attempt inserting negative cache record: ${table_name}: ${pk_column_name}=${arg_insert_if_empty}" 1>&2
    runq "${database_file}" \
      ".parameter set @arg_insert_if_empty '${arg_insert_if_empty}'" \
      "BEGIN TRANSACTION;" \
      "INSERT OR IGNORE INTO \"${table_name}\" (
         \"${arg_primary_key_column%%:*}\",
         \"${arg_created_column%%:*}\",
         \"${arg_updated_column%%:*}\",
         \"${arg_deleted_column%%:*}\"
       ) VALUES (
         @arg_insert_if_empty,
         cast(strftime('%s') AS ${arg_created_column#*:}),
         cast(strftime('%s') AS ${arg_updated_column#*:}),
         cast(strftime('%s') AS ${arg_deleted_column#*:})
       );" \
      "UPDATE
         \"${table_name}\"
       SET
         \"${arg_updated_column%%:*}\" = cast(strftime('%s') AS ${arg_updated_column#*:}),
         \"${arg_deleted_column%%:*}\" = cast(strftime('%s') AS ${arg_deleted_column#*:})
       WHERE
         \"${arg_primary_key_column%%:*}\" = @arg_insert_if_empty;" \
      "COMMIT TRANSACTION;" 2>/dev/null || true
    exit 0
  fi
fi

import_table_name="__import__${table_name}"
csv_file="$(mktemp "${TMPDIR}/csv_file.XXXXXXXX")"
if ! jq --argjson current_column_specs "${current_column_specs:-[]}" --raw-output \
  'map(
    . as $item |
    $current_column_specs | map(
      .name as $column_name |
      .type as $column_type |
      if ("BLOB" == $column_type) then
        $item[$column_name]
      elif ("NUMERIC" == $column_type) then
        $item[$column_name]
      elif ("INTEGER" == $column_type) then
        $item[$column_name]
      elif ("JSON" == $column_type) then
        ($item[$column_name] | tojson)
      elif ("REAL" == $column_type) then
        $item[$column_name]
      elif ("TEXT" == $column_type) then
        $item[$column_name]
      else
        error("unsupported SQLite column data type: \($column_name):\($column_type)")
      end
    )
  )[] | @csv' < "${json_file}" > "${csv_file}"; then
  { echo "${0##*/}: failed to convert records into CSV format"
    echo
    echo "JSON:"
    head "${json_file}"
    echo
  } | error
  exit 1
fi

#
# Avoid importing records directly into the destination table to utilize `INSERT OR REPLACE`.
# Otherwise the import will be failing if there is some conflicting records in importing data.
#
runq_args=(
  "$(build_create_table_statement --temporary "${database_file}" "${import_table_name}" "${current_column_specs}")"
  ".mode csv"
  ".import '${csv_file}' ${import_table_name}"
  ".mode ascii"
  "BEGIN TRANSACTION;"
)

if [[ ${#arg_before_queries[@]} -gt 0 ]]; then
  for q in "${arg_before_queries[@]}"; do
    runq_args+=("${q}")
  done
fi

if [[ -n "${arg_preserve_created:-}" ]]; then
  if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_created_column:-}" ]]; then
    error "${0##*/}: you need to specify both --primary-key-column and --created-column to preserve created timestamp column"
    exit 1
  fi
  validate_ident "${arg_primary_key_column%%:*}" "${arg_created_column%%:*}"
  runq_args+=( \
"UPDATE
  \"${import_table_name}\"
SET
  \"${arg_created_column%%:*}\" = \"${table_name}\".\"${arg_created_column%%:*}\"
FROM
  \"${table_name}\"
WHERE
  \"${import_table_name}\".\"${arg_primary_key_column%%:*}\" = \"${table_name}\".\"${arg_primary_key_column%%:*}\";" \
)
fi

runq_args+=("$(build_insert_into_table_from_table_statement --insert-or-replace "${database_file}" "${table_name}" "${import_table_name}" "${current_column_specs}")")

if [[ -n "${arg_soft_delete:-}" ]]; then
  if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_deleted_column:-}" ]]; then
    error "${0##*/}: you need to specify both --primary-key-column and --deleted-column to perform soft delete"
    exit 1
  fi
  validate_ident "${arg_primary_key_column%%:*}" "${arg_deleted_column%%:*}"
  runq_args+=( \
"UPDATE
  \"${table_name}\"
SET
  \"${arg_deleted_column%%:*}\" = cast(strftime('%s') AS ${arg_deleted_column#*:})
WHERE
  \"${arg_deleted_column%%:*}\" < 0
  AND \"${arg_primary_key_column%%:*}\" NOT IN ( SELECT \"${arg_primary_key_column%%:*}\" FROM \"${import_table_name}\" );" \
)
fi

if [[ ${#arg_after_queries[*]} -gt 0 ]]; then
  for q in "${arg_after_queries[@]}"; do
    runq_args+=("${q}")
  done
fi

runq_args+=(
  "COMMIT TRANSACTION;"
  "SELECT COUNT(1) FROM \"${import_table_name}\";" # count up the number of imported records
)

cmdout="$(mktemp "${TMPDIR}/cmdout.XXXXXXXX")"
if runq "${database_file}" "${runq_args[@]}" 1>"${cmdout}"; then
  num_records="$(cat "${cmdout}")"
  if [[ -z "${DRY_RUN:-}" ]]; then
    info "${0##*/}: imported ${num_records:-0} record(s) into '${table_name}' table (${table_description:-n/a})."
  else
    info "${0##*/}: imported ${num_records:-0} record(s) into '${table_name}' table (${table_description:-n/a}) (dry-run)."
  fi
else
  { echo "${0##*/}: failed to insert records into '${table_name}' table (${table_description:-n/a})"
    echo
    echo "SQL:"
    for arg in "${runq_args[@]}"; do echo "${arg}"; done
    echo
    echo "CSV:"
    head "${csv_file}"
    echo
  } | error
  exit 1
fi
