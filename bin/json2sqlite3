#!/usr/bin/env bash

set -euo pipefail

[[ -z "${DEBUG:-}" ]] || set -x

TMPDIR="$(mktemp -d)"
VERSION="@@VERSION@@"
VERBOSITY=2

# set timeout as 60 seconds to work around with "database is locked" error
SQLITE_BUSY_TIMEOUT="${SQLITE_BUSY_TIMEOUT:-60000}"

on_exit() {
  rm -fr "${TMPDIR}"
}

trap on_exit EXIT

usage() {
  echo "Usage: ${0##*/} [OPTIONS] DATABASE_FILE TABLE_NAME [JSON_FILE]"
  echo
  case_pattern_tag=" ) # usage: "
  # try to extract `case` command patterns from given shell script
  case_patterns=()
  while IFS='' read -r line; do case_patterns+=("$line"); done < <(sed -e '/^[[:space:]]*#/d' "${BASH_SOURCE[0]}" | sed -n -e '/^while \[* \$# -gt 0 \]*; do$/,/^done$/p' | grep -F -h "${case_pattern_tag}")

  if [[ ${#case_patterns[*]} -gt 0 ]]; then
    echo "Options:"
    for case_pattern in "${case_patterns[@]+"${case_patterns[@]}"}"; do
      # extract `case` pattern expressions
      args_patterns="$(echo "${case_pattern%%"${case_pattern_tag}"*}" | tr -d '"' | tr -d "'" | tr -d ' ' | sed -e 's/|/, /g')"
      # extract help message prepared as comment
      args_message="${case_pattern#*"${case_pattern_tag}"}"
      if [[ "${#args_patterns}" -lt 22 ]]; then
        printf "  %-20s  %s\n" "${args_patterns:-}" "${args_message:-}"
      else
        printf "  %-s\n" "${args_patterns:-}"
        printf "                        %s\n" "${args_message:-}"
      fi
    done
    echo
  fi
}

error() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-0}" -ge 0 ]]; then
    { if [[ -t 2 ]]; then printf '\033[1;31m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

warn() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-1}" -ge 1 ]]; then
    { if [[ -t 2 ]]; then printf '\033[1;33m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

info() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-2}" -ge 2 ]]; then
    { if [[ -t 2 ]]; then printf '\033[0;32m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

debug() {
  if [[ -n "${VERBOSE:-}" ]] || [[ "${VERBOSITY:-3}" -ge 3 ]]; then
    { if [[ -t 2 ]]; then printf '\033[0;37m'; fi
      if [[ $# -eq 0 ]]; then cat; else printf '%s\n' "$*"; fi
      if [[ -t 2 ]]; then printf '\033[0m'; fi
    } 1>&2 || true
  fi
}

validate_ident() {
  local ident
  for ident in "$@"; do
    if [[ ! "${ident:-}" =~ ^[A-Z_a-z][0-9A-Z_a-z]*$ ]]; then
      error "${0##*/}: invalid identifier for SQLite3 table/column name: \"${ident:-}\""
      exit 1
    fi
  done
}

validate_type() {
  local ident
  for ident in "$@"; do
    case "$(tr '[:lower:]' '[:upper:]' <<< "${ident:-}")" in
    "BLOB" ) :;;
    "NUMERIC" | "NUM" ) :;;
    "INTEGER" | "INT" ) :;;
    "REAL" ) :;;
    "TEXT" | "JSON" ) :;;
    * )
      error "${0##*/}: unsupported SQLite3 data type: ${ident:-}" 1>&2
      exit 1
      ;;
    esac
  done
}

ppq() {
  shift 1 # skipping database file
  local arg # expecting SQL statements to follow...
  for arg in "$@"; do
    local continued_p=
    local line
    while IFS='' read -r line; do
      if [[ -z "${continued_p:-}" ]]; then
        echo "sqlite3> ${line:-}"
        if [[ "${line:-}" == "."* ]] || [[ "${line:-}" == *";" ]]; then
          :
        else
          continued_p=1
        fi
      else
        echo "    ...> ${line:-}"
      fi
    done <<< "${arg}"
  done
}

runq() {
  if [[ -n "${VERBOSE:-}" ]]; then
    ppq "$@" | debug
  fi
  if [[ -z "${DRY_RUN:-}" ]]; then
    sqlite3 -batch "$@"
  fi
}

compare_column_specs() {
  local control_column_specs="$1"
  local test_column_specs="$2"
  local cmdout
  cmdout="$(mktemp "${TMPDIR}/cmdout.XXXXXXXX")"
  local control_column_names
  local test_column_names
  control_column_names="$(mktemp "${TMPDIR}/tmp.XXXXXXXX")"
  test_column_names="$(mktemp "${TMPDIR}/tmp.XXXXXXXX")"
  jq --raw-output 'map([.name, .type, (if .pk == 1 then "PRIMARY KEY" else "" end)]) | sort[] | @tsv' <<< "${control_column_specs}" > "${control_column_names}"
  jq --raw-output 'map([.name, .type, (if .pk == 1 then "PRIMARY KEY" else "" end)]) | sort[] | @tsv' <<< "${test_column_specs}" > "${test_column_names}"
  if diff -u "${control_column_names}" "${test_column_names}"; then
    :
  else
    exit_status="$?"
    { echo "${0##*/:}: detected schema drift on table column(s)..."
      cat "${cmdout}"
    } | info
    return "${exit_status:-1}"
  fi
}

detect_column_specs_from_args() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local arg
  local cid=1
  local specs='[]'
  for arg in "$@"; do
    local column_name="${arg%%:*}"
    local column_type="${arg#*:}"
    validate_ident "${column_name}"
    local pk='false'
    if [[ "${column_name:-}" == "${pk:-}" ]]; then
      pk='true'
    fi
    specs="$(jq \
      --argjson cid "${cid}" \
      --argjson notnull 'false' \
      --argjson dflt_value 'null' \
      --argjson pk "${pk:-false}" \
      --arg name "${column_name}" \
      --arg type "${column_type}" \
      --compact-output \
      '. + [{cid: $cid, name: $name, type: $type, notnull: $notnull, dflt_value: $dflt_value, pk: $pk}]' \
      <<< "${specs}" \
    )"
    cid="$(( cid + 1 ))"
  done
  echo "${specs}"
}

detect_column_specs_from_json() {
  local json_file="$1"
  local json_file_length=0
  json_file_length="$(jq --raw-output 'length' < "${json_file}")"
  if [[ "${json_file_length:-0}" -eq 0 ]]; then
    echo '[]'
  else
    local csv_file
    csv_file="$(mktemp "${TMPDIR}/csv_file.XXXXXXXX")"
    jq --raw-output 'map(
      to_entries | map(
        (.value | type) as $json_type |
          (if "array" == $json_type then
            "JSON"
          elif "boolean" == $json_type then
            "INTEGER"
          elif "null" == $json_type then
            "JSON"
          elif "number" == $json_type then
            "NUMERIC"
          elif "object" == $json_type then
            "JSON"
          elif "string" == $json_type then
            "TEXT"
          else
            error("unsupported JSON data type: \(.value):\($json_type)")
          end) as $sqlite_type | [.key, $sqlite_type]
        )
    ) | if 0 < length then add[] else [] end | @csv' < "${json_file}" > "${csv_file}"
    sqlite3 -batch "/dev/null" \
      ".mode list" \
      ".timeout ${SQLITE_BUSY_TIMEOUT}" \
      "CREATE TEMPORARY TABLE _s1 (name TEXT, type TEXT);" \
      "CREATE TEMPORARY TABLE s1 (cid INTEGER PRIMARY KEY, name TEXT, type TEXT);" \
      "CREATE UNIQUE INDEX s1_name ON s1 (name);" \
      ".mode csv" \
      ".import '${csv_file}' _s1" \
      ".mode list" \
      "INSERT OR IGNORE INTO s1 (name, type) SELECT name, type FROM _s1 WHERE 0 < length(name);" \
      "SELECT json_group_array(json_object('cid', cid, 'name', name, 'type', type, 'notnull', FALSE, 'dflt_value', NULL, 'pk', FALSE)) FROM s1 ORDER BY cid;"
  fi
}

detect_column_specs_from_table() {
  local database_file="$1"
  local table_name="$2"
  sqlite3 -batch "${database_file}" \
    ".mode list" \
    ".timeout ${SQLITE_BUSY_TIMEOUT}" \
    ".parameter set @table_name '${table_name}'" \
    "SELECT json_group_array(json_object('cid', cid, 'name', name, 'type', type, 'notnull', \"notnull\", 'dflt_value', dflt_value, 'pk', pk)) FROM pragma_table_info(@table_name) ORDER BY cid;"
}

build_create_table_statement() {
  local pk
  local temporary_p
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    "--temporary" ) temporary_p=1 ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local create_table_specs="$3"
  local stmt
  if [[ -z "${temporary_p:-}" ]]; then
    stmt="CREATE TABLE \"${table_name}\" ("
  else
    stmt="CREATE TEMPORARY TABLE \"${table_name}\" ("
  fi
  local _column
  local _columns=()
  while IFS='' read -r _column; do
    validate_ident "${_column%%:*}"
    validate_type "${_column#*:}"
    _columns+=("${_column}")
  done < <(jq --raw-output 'map("\(.name):\(.type)")[]' <<< "${create_table_specs}")
  if [[ ${#_columns[@]} -eq 0 ]]; then
    echo "${0##*/}: empty table column information" 1>&2
    return 1
  else
    for _column in "${_columns[@]}"; do
      column_name="${_column%%:*}"
      column_type="${_column#*:}"
      validate_ident "${column_name}"
      validate_type "${column_type}"
      if [[ "${column_name}" == "${pk:-}" ]]; then
        column_type+=" PRIMARY KEY"
      fi
      stmt+=$'\n'
      stmt+="  \"${column_name}\" ${column_type},"
    done
    stmt="${stmt%,}"
    stmt+=$'\n)'
    echo "${stmt};"
  fi
}

build_insert_into_table_from_table_statement() {
  local insert_or_replace_p
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--insert-or-replace" ) insert_or_replace_p=1 ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local source_table_name="$3"
  local table_column_specs="$4"
  local stmt
  if [[ -n "${insert_or_replace_p:-}" ]]; then
    stmt="INSERT OR REPLACE INTO \"${table_name}\" ("
  else
    stmt="INSERT INTO \"${table_name}\" ("
  fi
  local _column
  local _columns=()
  while IFS='' read -r _column; do
    validate_ident "${_column%%:*}"
    validate_type "${_column#*:}"
    _columns+=("${_column}")
  done < <(jq --raw-output 'map("\(.name):\(.type)")[]' <<< "${table_column_specs}")
  if [[ ${#_columns[*]} -eq 0 ]]; then
    echo "${0##*/}: empty table column information" 1>&2
    return 1
  else
    for _column in "${_columns[@]}"; do
      stmt+=$'\n'
      stmt+="  \"${_column%%:*}\","
    done
    stmt="${stmt%,}"
    stmt+=$'\n) SELECT'
    for _column in "${_columns[@]}"; do
      stmt+=$'\n'
      #
      # Converting internal representation of NULL into SQLite3 NULL.
      # SQLite3's `.import` doesn't support importing NULL values from CSV, but replaces them with empty strings.
      # This is a workaround to import NULL values from CSV by utilizing internal representation w/ magic bytes.
      #
      # See:
      # * https://sqlite.org/forum/info/cf99368fe4c4512e
      # * https://stackoverflow.com/questions/36774228/sqlite3-how-to-import-null-values-from-csv
      #
      stmt+="  ( CASE WHEN \"${_column%%:*}\" = (char(0xde, 0xad, 0xbe, 0xef) || 'NULL') THEN NULL ELSE \"${_column%%:*}\" END ),"
    done
    stmt="${stmt%,}"
    stmt+=$'\nFROM\n'
    stmt+="  \"${source_table_name}\";"
    echo "${stmt}"
  fi
}

reconcile_column_specs3() {
  local spec1="$1"
  local spec2="$2"
  local spec3="$3"
  local s1
  local s2
  local s3
  s1="$(mktemp "${TMPDIR}/s1.XXXXXXXX")"
  s2="$(mktemp "${TMPDIR}/s2.XXXXXXXX")"
  s3="$(mktemp "${TMPDIR}/s3.XXXXXXXX")"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec1}" > "${s1}"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec2}" > "${s2}"
  jq --raw-output 'map([.cid, .name, .type, .notnull, .dflt_value, .pk])[] | @csv' <<< "${spec3}" > "${s3}"
  sqlite3 -batch /dev/null \
    ".mode list" \
    ".timeout ${SQLITE_BUSY_TIMEOUT}" \
    "CREATE TEMPORARY TABLE s1 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    "CREATE TEMPORARY TABLE s2 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    "CREATE TEMPORARY TABLE s3 (cid INTEGER, name TEXT, type TEXT, \"notnull\" INTEGER, dflt_value BLOB, pk INTEGER);" \
    ".mode csv" \
    ".import '${s1}' s1" \
    ".import '${s2}' s2" \
    ".import '${s3}' s3" \
    ".mode list" \
    "WITH names AS (
      SELECT
        name
      FROM
        s1
      WHERE
        name IS NOT NULL AND name != ''
      UNION SELECT
        name
      FROM
        s2
      WHERE
        name IS NOT NULL AND name != ''
      UNION SELECT
        name
      FROM
        s3
      WHERE
        name IS NOT NULL AND name != ''
    ), s0 AS (
      SELECT
        ( CASE
          WHEN s1.cid IS NOT NULL THEN s1.cid
          WHEN s2.cid IS NOT NULL THEN s2.cid + ( SELECT COUNT(1) FROM s1 )
          WHEN s3.cid IS NOT NULL THEN s3.cid + ( SELECT COUNT(1) FROM s1 ) + ( SELECT COUNT(1) FROM s2 )
          ELSE ( SELECT COUNT(1) FROM s1 ) + ( SELECT COUNT(1) FROM s2 ) + ( SELECT COUNT(1) FROM s3 )
          END ) AS cid,
        names.name,
        ( CASE
          WHEN s1.type IS NOT NULL AND s1.type != '' THEN s1.type
          WHEN s2.type IS NOT NULL AND s2.type != '' THEN s2.type
          WHEN s3.type IS NOT NULL AND s3.type != '' THEN s3.type
          ELSE 'JSON'
          END ) AS type,
        ( CASE
          WHEN s1.\"notnull\" IS NOT NULL THEN s1.\"notnull\"
          WHEN s2.\"notnull\" IS NOT NULL THEN s2.\"notnull\"
          WHEN s3.\"notnull\" IS NOT NULL THEN s3.\"notnull\"
          ELSE FALSE
          END ) AS \"notnull\",
        ( CASE
          WHEN s1.dflt_value IS NOT NULL THEN s1.dflt_value
          WHEN s2.dflt_value IS NOT NULL THEN s2.dflt_value
          WHEN s3.dflt_value IS NOT NULL THEN s3.dflt_value
          ELSE NULL
          END ) AS dflt_value,
        ( CASE
          WHEN s1.pk IS NOT NULL THEN s1.pk
          WHEN s2.pk IS NOT NULL THEN s2.pk
          WHEN s3.pk IS NOT NULL THEN s3.pk
          ELSE FALSE
          END ) AS pk
      FROM
        names
      LEFT JOIN
        s1
          ON names.name = s1.name
      LEFT JOIN
        s2
          ON names.name = s2.name
      LEFT JOIN
        s3
          ON names.name = s3.name
      ORDER BY
        cid
    ) SELECT
      json_group_array(
        json_object(
          'cid', cid,
          'name', name,
          'type', type,
          'notnull', \"notnull\",
          'dflt_value', dflt_value,
          'pk', pk
        )
      )
    FROM
      s0;"
}

alter_table() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local old_column_specs="$3"
  local new_column_specs="$4"

  local backup_table_name
  backup_table_name="__backup$(date '+%s')__${table_name}"
  runq_args=( \
    ".mode list" \
    ".timeout ${SQLITE_BUSY_TIMEOUT}" \
    "BEGIN TRANSACTION;" \
    "DROP TABLE IF EXISTS \"${backup_table_name}\";" \
    "ALTER TABLE \"${table_name}\" RENAME TO \"${backup_table_name}\";" \
  )
  runq_args+=("$(build_create_table_statement --pk="${pk:-}" "${database_file}" "${table_name}" "${new_column_specs}")")
  runq_args+=("$(build_insert_into_table_from_table_statement --insert-or-replace "${database_file}" "${table_name}" "${backup_table_name}" "${old_column_specs}")")
  runq_args+=("DROP TABLE IF EXISTS \"${backup_table_name}\";") # TODO: add an option to opt-out this...?
  runq_args+=("COMMIT TRANSACTION;")
  runq "${database_file}" "${runq_args[@]}"
}

create_table() {
  local pk
  while [[ $# -gt 0 ]]; do
    case "${1:-}" in
    "--pk="* ) pk="${1#*=}" ;;
    * ) break ;;
    esac
    shift 1
  done
  local database_file="$1"
  local table_name="$2"
  local table_column_specs="$3"
  local stmt
  stmt="$(build_create_table_statement --pk="${pk:-}" "${database_file}" "${table_name}" "${table_column_specs}" || true)"
  if [[ -z "${stmt:-}" ]]; then
    return 1
  fi
  runq "${database_file}" \
    ".mode list" \
    ".timeout ${SQLITE_BUSY_TIMEOUT}" \
    "${stmt}"
}

arg_after_queries=()
arg_alter_table=1
arg_before_queries=()
arg_column_specs=() # contains columns including special columns e.g. primary key, created/updated/deleted
arg_create_table=1
arg_created_column=
arg_database_file=
arg_deleted_column=
arg_dump_intermediate_csv=
arg_insert_if_empty=
arg_json_files=()
arg_preserve_created=
arg_primary_key_column=
arg_soft_delete=
arg_table_name=
arg_updated_column=

while [[ $# -gt 0 ]]; do
  case "${1:-}" in
  "--" )
    shift 1
    break
    ;;
  "--after-query" | "--after-query="* )
    if [[ "$1" == *"="* ]]; then
      arg_after_queries+=("${1#*=}")
    else
      arg_after_queries+=("${2:-}")
      shift 1
    fi
    ;;
  "--alter-table" | "--no-alter-table" ) # usage: enable/disable ALTER TABLE operation (default: enabled)
    if [[ "$1" == "--no-"* ]]; then
      arg_alter_table=
    else
      arg_alter_table=1
    fi
    ;;
  "--before-query" | "--before-query="* )
    if [[ "$1" == *"="* ]]; then
      arg_before_queries+=("${1#*=}")
    else
      arg_before_queries+=("${2:-}")
      shift 1
    fi
    ;;
  "--create-table" | "--no-create-table" ) # usage: enable/disable CREATE TABLE operation (default: enabled)
    if [[ "$1" == "--no-"* ]]; then
      arg_create_table=
    else
      arg_create_table=1
    fi
    ;;
  "--created-column" | "--created-column="* ) # usage: specify a column name to contain creation timestamp. you also can specify column type (e.g. created_at:INTEGER)
    if [[ "$1" == *"="* ]]; then
      arg_created_column="${1#*=}"
    else
      arg_created_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_created_column:-}" != *":"* ]]; then
      arg_created_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_created_column}")
    ;;
  "--debug" ) # usage: enable debug mode
    DEBUG=1
    set -x
    ;;
  "--deleted-column" | "--deleted-column="* ) # usage: specify a column name to contain deletion timestamp. you also can specify column type (e.g. deleted_at:INTEGER)
    if [[ "$1" == *"="* ]]; then
      arg_deleted_column="${1#*=}"
    else
      arg_deleted_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_deleted_column:-}" != *":"* ]]; then
      arg_deleted_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_deleted_column}")
    ;;
  "--dry-run" ) # usage: enable dry-run mode
    DRY_RUN=1
    ;;
  "--dump-intermediate-csv" | "--dump-intermediate-csv="* ) # usage: dump intermediate CSV at given filename
    if [[ "$1" == *"="* ]]; then
      arg_dump_intermediate_csv="${1#*=}"
    else
      arg_dump_intermediate_csv="${2:-}"
      shift 1
    fi
    ;;
  "--generic-column" | "--generic-column="* ) # usage: specify generic column. you also can specify column type (e.g. foo:TEXT)
    if [[ "$1" == *"="* ]]; then
      arg_generic_column="${1#*=}"
    else
      arg_generic_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_generic_column:-}" != *":"* ]]; then
      arg_generic_column+=":JSON"
    fi
    arg_column_specs+=("${arg_generic_column}")
    ;;
  "--help" | "-h" ) # usage: show this message
    usage
    exit 0
    ;;
  "--insert-if-empty" | "--insert-if-empty="* )
    if [[ "$1" == *"="* ]]; then
      arg_insert_if_empty="${1#*=}"
    else
      arg_insert_if_empty="${2:-}"
      shift 1
    fi
    ;;
  "--primary-key-column" | "--primary-key-column="* ) # usage: specify a column name to be used as primary key. you also can specify column type (e.g. id:INTEGER)
    if [[ "$1" == *"="* ]]; then
      arg_primary_key_column="${1#*=}"
    else
      arg_primary_key_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_primary_key_column:-}" != *":"* ]]; then
      arg_primary_key_column+=":JSON"
    fi
    arg_column_specs+=("${arg_primary_key_column}")
    ;;
  "--preserve-created" | "--no-preserve-created" )
    if [[ "$1" == "--no-"* ]]; then
      arg_preserve_created=
    else
      arg_preserve_created=1
    fi
    ;;
  "--soft-delete" | "--no-soft-delete" )
    if [[ "$1" == "--no-"* ]]; then
      arg_soft_delete=
    else
      arg_soft_delete=1
    fi
    ;;
  "--updated-column" | "--updated-column="* ) # usage: specify a column name to contain update timestamp. you also can specify column type (e.g. updated_at:INTEGER)
    if [[ "$1" == *"="* ]]; then
      arg_updated_column="${1#*=}"
    else
      arg_updated_column="${2:-}"
      shift 1
    fi
    if [[ "${arg_updated_column:-}" != *":"* ]]; then
      arg_updated_column+=":INTEGER"
    fi
    arg_column_specs+=("${arg_updated_column}")
    ;;
  "--version" ) # usage: display version information
    if [[ "${VERSION:-}" == "@@"* ]]; then
      VERSION="$(git describe --tags HEAD 2>/dev/null || true)"
    fi
    echo "${0##*/} ${VERSION:-n/a}"
    exit 0
    ;;
  "--verbose" | "-v" ) # usage: enable verbose mode. can be specified multiple times to increase verbosity
    VERBOSE=1
    VERBOSITY="$(( VERBOSITY + 1 ))"
    ;;
  "--"* | "-"* )
    { echo "${0##*/}: unrecognized argument: ${1:-}"
      echo
    } | error
    usage
    exit 1
    ;;
  * )
    if [[ -z "${arg_database_file:-}" ]]; then
      arg_database_file="${1:-}"
    elif [[ -z "${arg_table_name:-}" ]]; then
      if validate_ident "${1:-}"; then
        arg_table_name="${1:-}"
      else
        error "${0##*/}: invalid table name: ${1:-}"
        exit 1
      fi
    else
      if [[ -e "${1:-}" ]]; then
        arg_json_files+=("${1:-}")
      else
        error "${0##*/}: no such file: ${1:-}"
        exit 1
      fi
    fi
    ;;
  esac
  shift 1
done

if ! command -v jq 1>/dev/null 2>&1; then
  error "${0##*/}: jq is not available"
  exit 1
fi

if ! command -v sqlite3 1>/dev/null 2>&1; then
  error "${0##*/}: sqlite3 is not available"
  exit 1
fi

if [[ -z "${arg_database_file:-}" ]]; then
  { echo "${0##*/}: database file was not given"
    echo
  } | error
  usage
  exit 1
fi

if [[ -z "${arg_table_name:-}" ]]; then
  { echo "${0##*/}: table name was not given"
    echo
  } | error
  usage
  exit 1
fi

if [[ $# -gt 0 ]]; then
  arg_json_files+=("$@")
fi

if [[ -z "${arg_primary_key_column:-}" ]]; then
  pk_column_name=
else
  pk_column_name="${arg_primary_key_column%%:*}" # extract type info
  validate_ident "${pk_column_name}"
  validate_type "${arg_primary_key_column#*:}"
fi

# read json from remainder of command line arguments, or standard input
json_file="$(mktemp "${TMPDIR}/json_file.XXXXXXXX")"
if [[ ${#arg_json_files[*]} -eq 0 ]]; then
  cat > "${json_file}"
else
  cat -- "${arg_json_files[@]}" > "${json_file}"
fi

# fail early if the input data is not well-formed
json_file_length=0
if [[ -s "${json_file}" ]]; then
  # SQLite3's 'json' mode will produce records in array<object> schema.
  # Here this is expecting that the input data is structured in the same way.
  json_file_type="$(jq --raw-output 'type' < "${json_file}" 2>/dev/null || true)"
  if [[ "${json_file_type:-}" == "array" ]]; then
    json_file_length="$(jq --raw-output 'length' < "${json_file}" 2>/dev/null || true)"
  else
    error "${0##*/}: unexpected input data format: json: expected=array, got=${json_file_type:-unknown}"
    exit 1
  fi
else
  json_file_length=0
fi

# managing table schema...
table_description=""
reconciled_column_specs='[]'
if sqlite3 -batch "${arg_database_file}" ".mode list" ".timeout ${SQLITE_BUSY_TIMEOUT}" "SELECT 1 FROM \"${arg_table_name}\" LIMIT 1;" 1>/dev/null 2>&1; then
  debug "${0##*/}: detected existing table. inspecting schema information..."
  current_column_specs="$(detect_column_specs_from_table "${arg_database_file}" "${arg_table_name}" || true)"
  if [[ -z "${current_column_specs:-}" ]]; then
    error "${0##*/}: failed to inspect schema information from existing table: ${arg_table_name}"
    exit 1
  fi
  if [[ ${#arg_column_specs[*]} -eq 0 ]]; then
    specified_column_specs='[]'
  else
    specified_column_specs="$(detect_column_specs_from_args --pk="${pk_column_name:-}" "${arg_column_specs[@]}" || true)"
    if [[ -z "${specified_column_specs:-}" ]]; then
      error "${0##*/}: failed to parse column specifications from arguments"
      exit 1
    fi
  fi
  detected_column_specs="$(detect_column_specs_from_json "${json_file}" || true)"
  if [[ -z "${detected_column_specs:-}" ]]; then
    error "${0##*/}: failed to detect column specifications from input data"
    exit 1
  fi
  reconciled_column_specs="$(reconcile_column_specs3 "${current_column_specs}" "${specified_column_specs}" "${detected_column_specs}" || true)"
  if [[ -z "${reconciled_column_specs:-}" ]]; then
    error "${0##*/}: failed to reconcile conflict on column specifications"
    exit 1
  else
    reconciled_column_specs_length="$(jq --raw-output 'length' <<< "${reconciled_column_specs:-[]}")"
    if [[ "${reconciled_column_specs_length:-0}" -eq 0 ]]; then
      error "${0##*/}: empty table column information for '${arg_table_name}'"
      exit 1
    fi
  fi
  if compare_column_specs "${current_column_specs}" "${reconciled_column_specs}"; then
    table_description="existing table"
    info "${0##*/}: existing table '${arg_table_name}' has schema compatible with columns detected in importing data."
  else
    if [[ -z "${arg_alter_table:-}" ]]; then
      error "${0##*/}: existing table '${arg_table_name}' has schema drift with columns detected in importing data. use --alter-table option to alter table."
      exit 1
    else
      table_description="existing table with modifying schema"
      warn "${0##*/}: existing table '${arg_table_name}' has schema drift with columns detected in importing data. will alter table..."
      if ! alter_table --pk="${pk_column_name:-}" "${arg_database_file}" "${arg_table_name}" "${current_column_specs}" "${reconciled_column_specs}"; then
        error "${0##*/}: failed to alter table: ${arg_table_name}"
        exit 1
      fi
    fi
  fi
else
  debug "${0##*/}: not detected existing table. preparing schema information to create new table..."
  if [[ -z "${arg_create_table:-}" ]]; then
    error "${0##*/}: table '${arg_table_name}' does not exist. use --create-table option to create table."
    exit 1
  else
    table_description="new table"
    if [[ ${#arg_column_specs[*]} -eq 0 ]]; then
      specified_column_specs='[]'
    else
      specified_column_specs="$(detect_column_specs_from_args --pk="${pk_column_name:-}" "${arg_column_specs[@]}" || true)"
      if [[ -z "${specified_column_specs:-}" ]]; then
        error "${0##*/}: failed to parse column specifications from arguments"
        exit 1
      fi
    fi
    detected_column_specs="$(detect_column_specs_from_json "${json_file}" || true)"
    if [[ -z "${detected_column_specs:-}" ]]; then
      error "${0##*/}: failed to detect column specifications from input data"
      exit 1
    fi
    reconciled_column_specs="$(reconcile_column_specs3 '[]' "${specified_column_specs}" "${detected_column_specs}" || true)"
    if [[ -z "${reconciled_column_specs:-}" ]]; then
      error "${0##*/}: failed to reconcile conflict on column specifications"
      exit 1
    else
      reconciled_column_specs_length="$(jq --raw-output 'length' <<< "${reconciled_column_specs:-[]}")"
      if [[ "${reconciled_column_specs_length:-0}" -eq 0 ]]; then
        error "${0##*/}: empty table column information for '${arg_table_name}'"
        exit 1
      fi
    fi
    warn "${0##*/}: creating new table '${arg_table_name}'"
    if ! create_table --pk="${pk_column_name:-}" "${arg_database_file}" "${arg_table_name}" "${reconciled_column_specs}"; then
      error "${0##*/}: failed to create table: ${arg_table_name}"
      exit 1
    fi
  fi
fi

if [[ -z "${DRY_RUN:-}" ]]; then
  current_column_specs="$(detect_column_specs_from_table "${arg_database_file}" "${arg_table_name}" || true)"
  if [[ -z "${current_column_specs:-}" ]]; then
    error "${0##*/}: failed to inspect table schema: ${arg_table_name}"
    exit 1
  fi
else
  current_column_specs="${reconciled_column_specs}"
fi

if [[ "${json_file_length:-0}" -eq 0 ]]; then
  if [[ -z "${arg_insert_if_empty:-}" ]]; then
    info "${0##*/}: empty file. skipping importing: ${json_file}"
    exit 0
  else
    if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_created_column:-}" ]] || [[ -z "${arg_updated_column:-}" ]] || [[ -z "${arg_deleted_column:-}" ]]; then
      error "${0##*/}: you need to specify --primary-key-column, --created-column, --updated-column and --deleted-column to insert negative cache record"
      exit 1
    fi
    validate_ident "${arg_primary_key_column%%:*}" "${arg_created_column%%:*}" "${arg_updated_column%%:*}" "${arg_deleted_column%%:*}"
    validate_type "${arg_primary_key_column#*:}" "${arg_created_column#*:}" "${arg_updated_column#*:}" "${arg_deleted_column#*:}"
    info "${0##*/}: empty file. attempt inserting negative cache record: ${arg_table_name}: ${pk_column_name}=${arg_insert_if_empty}" 1>&2
    runq_args=( \
      ".mode list" \
      ".timeout ${SQLITE_BUSY_TIMEOUT}" \
      ".parameter set @arg_insert_if_empty '${arg_insert_if_empty}'" \
      "BEGIN TRANSACTION;" \
    )

    if [[ ${#arg_before_queries[@]} -gt 0 ]]; then
      i=1
      for q in "${arg_before_queries[@]}"; do
        runq_args+=( \
          "${q}" \
          "SELECT @program_name || ': mutated ' || changes() || ' record(s) on before-query#${i}.' WHERE 0 < changes();" \
        )
        i=$((i+1))
      done
    fi

    runq_args+=( \
      "INSERT OR IGNORE INTO \"${arg_table_name}\" (
         \"${arg_primary_key_column%%:*}\",
         \"${arg_created_column%%:*}\",
         \"${arg_updated_column%%:*}\",
         \"${arg_deleted_column%%:*}\"
       ) VALUES (
         @arg_insert_if_empty,
         cast(strftime('%s') AS ${arg_created_column#*:}),
         cast(strftime('%s') AS ${arg_updated_column#*:}),
         cast(strftime('%s') AS ${arg_deleted_column#*:})
       );" \
      "UPDATE
         \"${arg_table_name}\"
       SET
         \"${arg_updated_column%%:*}\" = cast(strftime('%s') AS ${arg_updated_column#*:}),
         \"${arg_deleted_column%%:*}\" = cast(strftime('%s') AS ${arg_deleted_column#*:})
       WHERE
         \"${arg_primary_key_column%%:*}\" = @arg_insert_if_empty;" \
    )

    if [[ ${#arg_after_queries[*]} -gt 0 ]]; then
      i=1
      for q in "${arg_after_queries[@]}"; do
        runq_args+=( \
          "${q}" \
          "SELECT @program_name || ': mutated ' || changes() || ' record(s) on after-query#${i}.' WHERE 0 < changes();" \
        )
        i=$((i+1))
      done
    fi
    runq_args+=( \
      "COMMIT TRANSACTION;" \
    )
    runq "${arg_database_file}" "${runq_args[@]}" || true
    exit 0
  fi
fi

csv_file="$(mktemp "${TMPDIR}/csv_file.XXXXXXXX")"
if jq --argjson current_column_specs "${current_column_specs:-[]}" --raw-output \
  'map(
    . as $item |
    $current_column_specs | map(
      .name as $column_name |
        (.type | ascii_upcase) as $column_type |
      if ($item[$column_name] | type == "null") then
        "\u00de\u00ad\u00be\u00efNULL" # converting JSON null into internal representation of NULL (w/ magic bytes) to insert SQLite3 NULL from CSV...
      else
        if ("BLOB" == $column_type) then
          $item[$column_name]
        elif ("NUMERIC" == $column_type or "NUM" == $column_type) then
          $item[$column_name]
        elif ("INTEGER" == $column_type or "INT" == $column_type) then
          if ($item[$column_name] | type == "boolean") then
            if $item[$column_name] then
              1
            else
              0
            end
          else
            $item[$column_name]
          end
        elif ("JSON" == $column_type) then
          ($item[$column_name] | tojson)
        elif ("REAL" == $column_type) then
          $item[$column_name]
        elif ("TEXT" == $column_type) then
          $item[$column_name]
        else
          error("unsupported SQLite data type: \($column_name):\($column_type)")
        end
      end
    )
  )[] | @csv' < "${json_file}" > "${csv_file}"; then
  if [[ -z "${arg_dump_intermediate_csv:-}" ]]; then
    :
  else
    { jq --raw-output 'map("\(.name):\(.type)") | @csv' <<< "${current_column_specs:-[]}"
      cat "${csv_file}"
    } > "${arg_dump_intermediate_csv}"
  fi
else
  { echo "${0##*/}: failed to convert records into CSV format"
    echo
    echo "JSON:"
    head "${json_file}"
    echo
  } | error
  exit 1
fi

#
# Avoid importing records directly into the destination table to utilize `INSERT OR REPLACE`.
# Otherwise the import will be failing if there is some conflicting records in importing data.
#
import_table_name="__import__${arg_table_name}"
runq_args=( \
  ".mode list" \
  ".timeout ${SQLITE_BUSY_TIMEOUT}" \
  "$(build_create_table_statement --temporary "${arg_database_file}" "${import_table_name}" "${current_column_specs}")" \
  ".parameter set @program_name '${0##*/}'" \
  ".parameter set @arg_table_name '${arg_table_name}'" \
  ".parameter set @table_description '${table_description:-n/a}'" \
  ".mode csv" \
  ".import '${csv_file}' ${import_table_name}" \
  ".mode list" \
  "BEGIN TRANSACTION;" \
)

if [[ ${#arg_before_queries[@]} -gt 0 ]]; then
  i=1
  for q in "${arg_before_queries[@]}"; do
    runq_args+=( \
      "${q}" \
      "SELECT @program_name || ': mutated ' || changes() || ' record(s) on before-query#${i}.' WHERE 0 < changes();" \
    )
    i=$((i+1))
  done
fi

if [[ -n "${arg_preserve_created:-}" ]]; then
  if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_created_column:-}" ]]; then
    error "${0##*/}: you need to specify both --primary-key-column and --created-column to preserve created timestamp column"
    exit 1
  fi
  validate_ident "${arg_primary_key_column%%:*}" "${arg_created_column%%:*}"
  validate_type "${arg_primary_key_column#*:}" "${arg_created_column#*:}"
  runq_args+=( \
"UPDATE
  \"${import_table_name}\"
SET
  \"${arg_created_column%%:*}\" = \"${arg_table_name}\".\"${arg_created_column%%:*}\"
FROM
  \"${arg_table_name}\"
WHERE
  \"${import_table_name}\".\"${arg_primary_key_column%%:*}\" = \"${arg_table_name}\".\"${arg_primary_key_column%%:*}\";" \
"SELECT @program_name || ': updated ' || changes() || ' record(s) on preserve-created.' WHERE changes();" \
)
fi

runq_args+=( \
  "$(build_insert_into_table_from_table_statement --insert-or-replace "${arg_database_file}" "${arg_table_name}" "${import_table_name}" "${current_column_specs}")" \
  "SELECT @program_name || ': imported ' || changes() || ' record(s) into ''' || @arg_table_name || ''' table (' || @table_description || ').' WHERE 0 < changes();" \
)

if [[ -n "${arg_soft_delete:-}" ]]; then
  if [[ -z "${arg_primary_key_column:-}" ]] || [[ -z "${arg_deleted_column:-}" ]]; then
    error "${0##*/}: you need to specify both --primary-key-column and --deleted-column to perform soft delete"
    exit 1
  fi
  validate_ident "${arg_primary_key_column%%:*}" "${arg_deleted_column%%:*}"
  validate_type "${arg_primary_key_column#*:}" "${arg_deleted_column#*:}"
  runq_args+=( \
"UPDATE
  \"${arg_table_name}\"
SET
  \"${arg_deleted_column%%:*}\" = cast(strftime('%s') AS ${arg_deleted_column#*:})
WHERE
  \"${arg_deleted_column%%:*}\" < 0
  AND \"${arg_primary_key_column%%:*}\" NOT IN ( SELECT \"${arg_primary_key_column%%:*}\" FROM \"${import_table_name}\" );" \
"SELECT @program_name || ': updated ' || changes() || ' record(s) on soft-delete.' WHERE 0 < changes();" \
)
fi

if [[ ${#arg_after_queries[*]} -gt 0 ]]; then
  i=1
  for q in "${arg_after_queries[@]}"; do
    runq_args+=( \
      "${q}" \
      "SELECT @program_name || ': mutated ' || changes() || ' record(s) on after-query#${i}.' WHERE 0 < changes();" \
    )
    i=$((i+1))
  done
fi

runq_args+=( \
  "COMMIT TRANSACTION;" \
)

cmdout="$(mktemp "${TMPDIR}/cmdout.XXXXXXXX")"
if runq "${arg_database_file}" "${runq_args[@]}" 1>"${cmdout}"; then
  # SQLite3 will render 0x1e (Record Separator) in between outputs
  if [[ -z "${DRY_RUN:-}" ]]; then
    tr '\036' '\n' < "${cmdout}" | info
  else
    info "${0##*/}: imported 0 record(s) into '${arg_table_name}' table (${table_description:-n/a}) (dry-run)."
  fi
else
  { echo "${0##*/}: failed to insert records into '${arg_table_name}' table (${table_description:-n/a})"
    echo
    echo "SQL:"
    for arg in "${runq_args[@]}"; do echo "${arg}"; done
    echo
    echo "CSV:"
    head "${csv_file}"
    echo
  } | error
  exit 1
fi
